{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitminercondaa6c31f2afe9143f993ded22a64766581",
   "display_name": "Python 3.7.6 64-bit ('miner': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTW Lab 15\n",
    "\n",
    "## David Josephs, Andy Heroy, Carson Drake, Che' Cobb\n",
    "\n",
    "### April 10, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For the final case study for DS7333, we were given an unlabeled dataset and tasked with providing insight and as to possible cost savings for a business partner. The dataset itself was 160,000 records with 51 unlabeled features.   Our only hint as to its domain is that its origins lie in the insurance industry.  The features themselves are labeled (x0-x49) with a binary target category ('y') of 0 or 1.  Our job is to apply machine learning modeling techniques to show a cost savings for our business partner for every correct classification of the target variable.  \n",
    "\n",
    "Seeing as we're dealing with a target variable that is binary, our job will be to select classification models that can best analyze the data.  The classification models we will attempt are as follows.  \n",
    "\n",
    "- Random Forrest\n",
    "- Logistic Regression\n",
    "- Something else\n",
    "\n",
    "## Background\n",
    "\n",
    "Put simply, a classification model trains a model on a sample of training data to then predict what the class of unseen test data.  The metrics for classification models are accuracy, precision, recall and F1-Score.  The main goal of this paper is to minimize false positives and false negatives.  As our business partner has kindly given us a \"cost\" function as to what false positives and negatives cost our company. \n",
    "\n",
    "- False positive = $ 10\n",
    "- False negative = $ 500\n",
    "- True pos/neg = $ 0\n",
    "\n",
    "When we evaluate our models, we implement a custom function to incorporate our business partners requirements in a cost function in the scoring section of the algorithm.  Sklearn comes with a handy function called make_scorer that allows us to monitor our models performance our partners cost savings in mind.  For a basic overview of other metrics that are standard in classification.  See Table 1 below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric | Description | Equation |\n",
    "|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------|\n",
    "| Accuracy | Accuracy is defined as the number of correct predictions divided  by the total number of predictions. | (True Positive + False Negatives)/Total amount of samples |\n",
    "| Precision | Precision is defined as the ratio of correctly predicted positives  to the total number of predicted positive observations. | True Positive/(True Positives + False Positives) |\n",
    "| Recall | Recall is defined as the ratio of correct positives to all the  observations in the class | True Positives/(True Positives + False Positives) |\n",
    "| F1-Score | F1 score is defined as the harmonic average of Precision and Recall.  This metric is more useful when you have uneven class balances but it  sometimes useful as it includes false postives and false negatives | 2x(Recall x Precision)/(Recall + Position) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messing with equations\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{acc} = \\left(\\mathop{\\sum_{j}\\sum_{i}}_{i=j} M\\right) / \\left(\\mathop{\\sum_{j}\\sum_{i}} M\\right)\\\\\n",
    "\\mathrm{precision} = \\mathrm{diag}(M) / \\left(\\mathop{\\sum_{j}} M\\right)\\\\\n",
    "\\mathrm{recall} = \\mathrm{diag}(M) / \\left(\\mathop{\\sum_{i}} M\\right)\\\\\n",
    "\\mathrm{f1} = \\frac{2(\\mathrm{precision})(\\mathrm{recall})}{\\mathrm{precision} + \\mathrm{recall}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "# TODO COME BACK AND FILL IN FINDINGS.  \n",
    "\n",
    "Report summary findings here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "\n",
    "Seeing as the data itself was unlabeled, we now begin the process of cleaningit in order to prepare it for modeling.  Below are our findings of the various properties of the dataset.\n",
    "\n",
    "1.  Datatypes:\n",
    "    - Numerical: 45 features\n",
    "    - Categorical: 5 features\n",
    "    - Boolean: 1 feature\n",
    "2.  Correlation:\n",
    "    - Two sets of columns show direct correlation.  \n",
    "    - x2 and x6\n",
    "    - x37 and x41\n",
    "    - Result: Due to perfect correlation the columns we decided to drop x2 and x41\n",
    "3.  Data Cleaning\n",
    "    - x24 contains country data.  \"euorpe\" was changed to \"europe\"\n",
    "    - x29 contains monthly data.\n",
    "        - \"Dev\" was changed to \"Dec\"\n",
    "        - \"sept.\" was changed to \"Sep\"\n",
    "    - x30 contains day of the week.  \"thurday\" changed to \"thursday\"\n",
    "    - x32 contains a percentage amount. All % signs were removed and datatype changed to float64\n",
    "    - x37 contains a dollar amount.  All $ were removed and the datatype was changed to float64\n",
    "4.  Missing Value's \n",
    "    - Each column look to have anywhere from 21 - 47 missing values.  At most, this comprises about 2.9% of the data.  Which is a small amount when compared to the full dataset.   Instead of dropping those rows, we imputed with the median for each numeric column.\n",
    "5.  Distribution\n",
    "    - After checking the histograms for each column, it was determined the data was normally distributed with no skew.  This leads us to believe that this dataset could have been generated with sklearns make_classification function.  To see the distributions check Figure XX below. \n",
    "6.  Scaling\n",
    "    - In order to make sure our data was scaled correctly for classification, we implemented sklearns StandardScaler function over the numerical columns. \n",
    "\n",
    "\n",
    "![Scaling](../plots/Histograms.png \"Histograms of Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "# TODO COME BACK AND TALK ABOUT CARSONS FEATURE ENGINEERING.  NOT SURE WHAT EXACTLY HE WAS DOING. \n",
    "\n",
    "Feature selection took our group quite some time as having unlabeled columns make it difficult to quantify what variables are important.  When this is the case, we rely on other methods to determine what columns are important for analysis and going back and forth of dropping them and running our analysis on various models. \n",
    "\n",
    "\n",
    "\n",
    "Our first attempt at this process was using Random Forest which comes with feature importance.  Upon the first run, we saw a fairly low variable scoring of importance within the dataset. Due to this, we decided to implement a \"Random\" column in to the analysis to see if random data was indeed more useful than the data.  Luckily, as you can see in the Figure XX below, the random column ranks fairly low in the importance so we can rest a little easier knowing that our data contains useful information.  \n",
    "\n",
    "### Random Forest Feature Importance\n",
    "\n",
    "![FeatImportance](../plots/BaselineRF_Feature_Imp.png \"Baseline Random Forest Feature Importance\")\n",
    "\n",
    "\n",
    "The most important features according to Random Forest > 0.02 were as follows. \n",
    "\n",
    "    - x23\n",
    "    - x20\n",
    "    - x48\n",
    "    - x49\n",
    "    - x42\n",
    "    - x12\n",
    "    - x28\n",
    "    - x27\n",
    "    - x40\n",
    "    - x37\n",
    "    - x7 \n",
    "    - x46 \n",
    "    - x41\n",
    "    - x38\n",
    "    - x2 \n",
    "    - x6 \n",
    "    - x32\n",
    "\n",
    "Leaving the remaining 33 variables available for dropping in re-running a random forest.  We didn't immediately drop these columns because we wanted to run Principal Component Analysis (PCA) to discover how many columns we needed to keep to maintain at least 95% variance. This type of dimensionality technique is very useful when you have unlabeled data such as this.  \n",
    "\n",
    "### Principal Compoment Analysis\n",
    "\n",
    "Below in Figure XX, we can see that in order to maintain 95% variance, we can select up to 36 different features to maintain our desired variance.  This is about twice as many features as with the random forest, but maintaining a conservative approach, we will use this limit to set our n_components for the next random forest on the reduced dataset.  \n",
    "\n",
    "![PCA](../plots/PCA.png \"Principal Component Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-9cacfa5da7c0>, line 4)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-9cacfa5da7c0>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    As we introduced it earlier,\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Modeling\n",
    "\n",
    "### Baseline Random Forrest\n",
    "As we introduced it earlier,\n",
    "\n",
    "### Random Forrest with PCA reduced dimensions\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}