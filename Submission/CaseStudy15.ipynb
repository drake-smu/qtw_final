{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitminercondaa6c31f2afe9143f993ded22a64766581",
   "display_name": "Python 3.7.6 64-bit ('miner': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTW Lab 15\n",
    "\n",
    "## David Josephs, Andy Heroy, Carson Drake, Che' Cobb\n",
    "\n",
    "### April 10, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For the final case study for DS7333, we were given an unlabeled dataset and tasked with providing insight and as to possible cost savings for a business partner. The dataset itself was 160,000 records with 51 unlabeled features.   Our only hint as to its domain is that its origins lie in the insurance industry.  The features themselves are labeled (x0-x49) with a binary target category ('y') of 0 or 1.  Our job is to apply machine learning modeling techniques to show a cost savings for our business partner for every correct classification of the target variable.  \n",
    "\n",
    "Seeing as we're dealing with a target variable that is binary, our job will be to select classification models that can best analyze the data.  The classification models we will attempt are as follows.  \n",
    "\n",
    "- Random Forrest\n",
    "- Logistic Regression\n",
    "- Something else\n",
    "\n",
    "## Background\n",
    "\n",
    "Put simply, a classification model trains a model on a sample of training data to then predict what the class of unseen test data.  The metrics for classification models are accuracy, precision, recall and F1-Score.  The main goal of this paper is to minimize false positives and false negatives.  As our business partner has kindly given us a \"cost\" function as to what false positives and negatives cost our company. \n",
    "\n",
    "- False positive = $ 10\n",
    "- False negative = $ 500\n",
    "- True pos/neg = $ 0\n",
    "\n",
    "When we evaluate our models, we implement a custom function to incorporate our business partners requirements in a cost function in the scoring section of the algorithm.  Sklearn comes with a handy function called make_scorer that allows us to monitor our models performance our partners cost savings in mind.  For a basic overview of other metrics that are standard in classification.  See Table 1 below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Accuracy | Accuracy is defined as the number of correct predictions divided  by the total number of predictions. | (True Positive + False Negatives)/Total amount of samples |\n",
    "|:---------:|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------:|\n",
    "| Precision | Precision is defined as the ratio of correctly predicted positives  to the total number of predicted positive observations. | True Positive/(True Positives + False Positives) |\n",
    "| Recall | Recall is defined as the ratio of correct positives to all the  observations in the class | True Positives/(True Positives + False Positives) |\n",
    "| F1-Score | F1 score is defined as the harmonic average of Precision and Recall.  This metric is more useful when you have uneven class balances but it  sometimes useful as it includes false postives and false negatives | 2x(Recall x Precision)/(Recall + Position) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messing with equations\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{acc} = \\left(\\mathop{\\sum_{j}\\sum_{i}}_{i=j} M\\right) / \\left(\\mathop{\\sum_{j}\\sum_{i}} M\\right)\\\\\n",
    "\\mathrm{precision} = \\mathrm{diag}(M) / \\left(\\mathop{\\sum_{j}} M\\right)\\\\\n",
    "\\mathrm{recall} = \\mathrm{diag}(M) / \\left(\\mathop{\\sum_{i}} M\\right)\\\\\n",
    "\\mathrm{f1} = \\frac{2(\\mathrm{precision})(\\mathrm{recall})}{\\mathrm{precision} + \\mathrm{recall}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "### Report summary findings here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "\n",
    "Seeing as the data itself was unlabeled, we now begin the process of cleaningit in order to prepare it for modeling.  Below are our findings of the various properties of the dataset.\n",
    "\n",
    "1.  Datatypes:\n",
    "    - Numerical: 45 features\n",
    "    - Categorical: 5 features\n",
    "    - Boolean: 1 features\n",
    "2.  Correlation:\n",
    "    - Two sets of columns show direct correlation.  \n",
    "    - x2 and x6\n",
    "    - x37 and x41\n",
    "    - Result: Due to perfect correlation the columns we decided to drop x2 and x41\n",
    "3.  Data Cleaning\n",
    "    - x24 contains country data.  \"euorpe\" was changed to \"europe\"\n",
    "    - x29 contains monthly data.\n",
    "        - \"Dev\" was changed to \"Dec\"\n",
    "        - \"sept.\" was changed to \"Sep\"\n",
    "    - x30 contains day of the week.  \"thurday\" changed to \"thursday\"\n",
    "    - x32 contains a percentage amount. All % signs were removed and datatype changed to float64\n",
    "    - x37 contains a dollar amount.  All $ were removed and the datatype was changed to float64\n",
    "4.  Missing Value's \n",
    "    - Each column look to have anywhere from 21 - 47 missing values.  At most, this comprises about 2.9% of the data.  Which is a small amount when compared to the full dataset.   Instead of dropping those rows, we imputed with the median for each numeric column.\n",
    "5.  Distribution\n",
    "    - After checking the histograms for each column, it was determined the data was normally distributed with no skew.  This leads us to believe that this dataset could have been generated with sklearns make_classification function.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n"
   ]
  }
 ]
}